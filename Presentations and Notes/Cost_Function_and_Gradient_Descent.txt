1. Cost function / loss function:
Basically cost function refers to loss or errors of our model. 
Errors are - if our model predicts some value but actual value of model is slightly different. So this diff. is error.
Cost function does the same, it measures the difference between actual output and predicted output from the model. So, it is measuring errors.
Cost function is average of errors in the data and Loss function is error for individual data points. E.g. If predicted o/p is 7 and actual is 7.20 then this diff. of individual data pt. is loss function. 
In case of errors of Entire Dataset, so that will be cost function.
When error is minimum then cost function is near to Global Minima. 

2. Types of errors of cost func. In ML:
There are 2 types of errors in cost function :
    1) Mean square error (MSE)
    2) Mean Absolute error (MAE)

Mean square error (MSE): Here, for every data point,
We find error between actual and predicted value 
And then squared it and take summation of those 
Values for each datapoint. Squaring because don’t 
want negative values. Here,
                        yi – yi^ -- difference 
                        n= total datapoints
                        i=1 --- started from 1 


Mean absolute error (MAE): Here same like MSE, for every data point,
    We find error between actual and predicted value 
    But instead of squaring it we take mod ( | | ) and take   summation of those values for each datapoint. Here,
                        yi – yi^ -- difference 
                        n= total datapoints
                        i=1 --- started from 1


3. What is Gradient descent?
Gradient descent is ML optimization algorithm helps to reduce the cost function so we can get models that predict accurate values.
Basically, it takes random values and by increasing or decreasing them GD tries to reach at global minima to predict accurate values. Our aim is to reach global minima with less errors.


4.How to find gradient descent?
Y = X2
If x = 6, y = 36
Y is big, so to reach at minima we have to find out that the value of x should increase or decrease to get minimum value of Y. 
So this can be done with derivative. So, Y = X2 
dy/dx = 2x   now ------ x = 6, y = 36
dy/dx = 2x = 2 * 6 = 12
Xnew = 6 – 0.1 * 12 = 4.8
Y = 23.04

Xnew = xold – learning rate * dy/dx
We can do this again until we get lower value of Y. Learning rate minimizes the steps.


5. Types of gradient descent:
1. Batch Gradient Descent
2. Stochastic Gradient Descent
3. Mini Batch Gradient Descent

1. Batch Gradient Descent : We use the entire dataset to calculate gradient descent of cost function for each iteration of GD.
It has easy to understands the analysis of weights and convergence rates.
But, if dataset is too high we can’t use BGD as it uses more memory, time consuming, costly and slow process as well because it uses whole dataset in one time.

2. Stochastic Gradient Descent : 
We use the single dataset to calc. gradient descent and update the weights with every iteration. By taking single dataset it’s accuracy also increases.
Learning is faster than BGD. Memory can be saved. 
Forcefully happening computation gets removed as we using only one dataset.
Weights can be updated more accurately for new data as we using only one dataset.
But, cost function fluctuates heavily as we frequently udate the weights.

3. Mini Batch Gradient Descent :
Here, we use mini batches of samples instead of using single dataset example or using whole dataset, this is middleway. 
Mini batch GD is used more widely and it converges faster and also more stable. Batch size can vary depending upon dataset.
And as we take mini batches, the variance of weights updates reduces and also help to have more coverage faster.
Speed also get increases due to small batches.
But, here loss is calculated for every batch, so it needs to be accumulated across all batches. 


